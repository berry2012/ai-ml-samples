{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66cb7681-176b-42fc-a1d0-46cb92ae4f2c",
   "metadata": {},
   "source": [
    "# Multimodal Algorithm for Misinformation Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af35682a-8da2-46be-a12e-bc33e836bf4b",
   "metadata": {},
   "source": [
    "Implementing a proof of concept (PoC) for a multimodal algorithm for misinformation detection requires integrating multiple data sources (text, images, videos, etc.) and leveraging AI/ML techniques to analyze and classify misinformation. \n",
    "\n",
    "Text Data: https://huggingface.co/datasets/ErfanMoosaviMonazzah/fake-news-detection-dataset-English\n",
    "\n",
    "Image Data: \n",
    "\n",
    "Video Data: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d48a475-4a49-431c-9576-2ad628805eba",
   "metadata": {},
   "source": [
    "**Install dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5e5485-2dd4-41fb-bb0f-6da2931c984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install opencv-python-headless\n",
    "#%conda install -c numba numba\n",
    "#%conda install -c conda-forge librosa\n",
    "#%pip install -U datasets huggingface_hub fsspec\n",
    "#%pip install torchaudio\n",
    "#%pip install librosa\n",
    "# pip uninstall torch torchvision transformers\n",
    "# pip install torch torchvision\n",
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c14576-4d36-49f2-a2aa-84fe1eda8be5",
   "metadata": {},
   "source": [
    "# 1. Data Collection & Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c39ac0-f9b3-4b04-9f21-f1782bef5eb4",
   "metadata": {},
   "source": [
    "## Text Analysis: Using transformer model (BERT) for fake news detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ce40b4-860e-48ce-9e80-9ba525888e51",
   "metadata": {},
   "source": [
    "### Explore FakeNewsNet Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ed63fc-eef5-4198-a6ac-9279773c27ce",
   "metadata": {},
   "source": [
    "**The FakeNewsNet dataset includes fake and real news articles along with metadata such as user engagement and network interactions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16be67a8-b7ce-41ec-90bd-0ef33eb57645",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load FakeNewsNet dataset from Hugging Face\n",
    "dataset = load_dataset(\"ErfanMoosaviMonazzah/fake-news-detection-dataset-English\")\n",
    "\n",
    "# Convert dataset to Pandas DataFrame\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "# Display sample\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nFakeNewsNet dataset explored successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4f71ce-6afc-4c97-8e48-beb1c7537837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of the number of words in the train data 'title'\n",
    "import matplotlib.pyplot as plt\n",
    "seq_len = [len(i.split()) for i in df['title']]\n",
    "pd.Series(seq_len).hist(bins = 40,color='firebrick')\n",
    "plt.xlabel('Number of words')\n",
    "plt.ylabel('Number of texts')\n",
    "plt.title('Histogram of Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b392695f-ccc0-4efc-884a-a7376502f61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data imbalance\n",
    "label_size = df.groupby('label').size()\n",
    "print(label_size)\n",
    "\n",
    "plt.pie(label_size, explode=[0.1,0.1], colors=['firebrick','blue'], startangle=90,labels=['Fake News', 'Real News'], shadow=True, autopct='%1.1f%%')\n",
    "plt.title('Data Imbalance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7362ca7e-3728-4976-b9d7-ae470ade8e10",
   "metadata": {},
   "source": [
    "## Text Data Preprocessing (BERT Tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceef1d27-2fdc-4450-862f-f7d314cbbb27",
   "metadata": {},
   "source": [
    "### How This Works\n",
    "\n",
    "âœ” Loads FakeNewsNet dataset\n",
    "\n",
    "âœ” Preprocesses text with BERT tokenization\n",
    "\n",
    "âœ” Creates PyTorch Dataloader for batch processing\n",
    "\n",
    "âœ” Fine-tunes BERT for Fake News classification\n",
    "\n",
    "âœ” Trains & evaluates the model with accuracy metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8140cd9-53b6-4a59-b47a-05603fd9f029",
   "metadata": {},
   "source": [
    "This imports BERT's tokenizer from the Hugging Face transformers library\n",
    "\n",
    "bert-base-uncased is a pre-trained BERT model that treats text as lowercase (e.g., \"Hello\" and \"hello\" are treated the same)\n",
    "\n",
    "Tokenize a sample data to what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5020a0-db7f-4ab3-be65-af702448ab5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def preprocess_text(texts):\n",
    "    \"\"\"\n",
    "    Tokenize a batch of texts with BERT.\n",
    "    \n",
    "    Args:\n",
    "    texts (list of str): List of text samples to tokenize.\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing input_ids, attention_mask, and token_type_ids.\n",
    "    \"\"\"\n",
    "    encoded_inputs = tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",  # Pad to max_length for batch consistency\n",
    "        truncation=True,       # Truncate texts longer than 20 tokens\n",
    "        max_length=15,        # BERT max token limit\n",
    "        return_tensors=\"pt\"    # Return PyTorch tensors\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": encoded_inputs[\"input_ids\"],\n",
    "        \"attention_mask\": encoded_inputs[\"attention_mask\"],\n",
    "        \"token_type_ids\": encoded_inputs[\"token_type_ids\"]\n",
    "    }\n",
    "\n",
    "# Example usage with a batch of texts\n",
    "sample_text = [\"Fake news detection using multimodal learning.\",\n",
    "               \"Using Bert.\"]\n",
    "\n",
    "# Tokenize and print results\n",
    "tokenized_output = preprocess_text(sample_text)\n",
    "print(tokenized_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4780fea-8a86-49c5-ad9b-f550dd198613",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8ccfdb-4fba-4734-be5c-dd603b0e57f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load dataset from Hugging Face\n",
    "dataset = load_dataset(\"ErfanMoosaviMonazzah/fake-news-detection-dataset-English\")\n",
    "\n",
    "# Select all the rows in training dataset\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n",
    "val_data = dataset[\"validation\"]\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Function to tokenize text\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=20)\n",
    "\n",
    "# Tokenize datasets\n",
    "train_data = train_data.map(tokenize_function, batched=True)\n",
    "test_data = test_data.map(tokenize_function, batched=True)\n",
    "val_data = val_data.map(tokenize_function, batched=True)\n",
    "\n",
    "# Convert labels to tensor format\n",
    "def format_dataset(data):\n",
    "    return data.with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "train_data = format_dataset(train_data)\n",
    "test_data = format_dataset(test_data)\n",
    "val_data = format_dataset(val_data)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_data, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=8)\n",
    "val_dataloader = DataLoader(val_data, batch_size=8)\n",
    "\n",
    "# Load BERT model with classification head\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Define learning rate scheduler\n",
    "num_training_steps = len(train_dataloader) * 3  # 3 epochs\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "# Define the loss functions\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Save model directory\n",
    "best_model_path = \"best_bert_fake_news_detector\"\n",
    "os.makedirs(best_model_path, exist_ok=True)\n",
    "\n",
    "# Initialize best accuracy tracking\n",
    "best_val_accuracy = 0.0\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(dataloader, split_name):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=f\"Evaluating on {split_name}\"):\n",
    "            batch = {\n",
    "                \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "                \"labels\": batch[\"label\"].to(device),  # Rename 'label' to 'labels'\n",
    "            }\n",
    "            outputs = model(**batch)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            correct += (predictions == batch[\"labels\"]).sum().item()\n",
    "            total += batch[\"labels\"].size(0)\n",
    "    accuracy = correct / total\n",
    "    print(f\"ðŸ“Š {split_name} Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Training loop with best model saving\n",
    "\n",
    "num_epochs = 3\n",
    "print(\"\\nðŸš€ Training Started...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    loop = tqdm(train_dataloader, leave=True)\n",
    "    \n",
    "    for batch in loop:\n",
    "        batch = {\n",
    "            \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "            \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "            \"labels\": batch[\"label\"].to(device),  # Rename 'label' to 'labels'\n",
    "        }\n",
    "        \n",
    "        outputs = model(**batch)  # No error now\n",
    "        loss = criterion(outputs.logits, batch[\"labels\"])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loop.set_description(f\"Epoch {epoch+1}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Evaluate after each epoch\n",
    "    val_accuracy = evaluate_model(val_dataloader, \"Validation\")\n",
    "\n",
    "    # Save the model if validation accuracy improves\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        model.save_pretrained(best_model_path)\n",
    "        tokenizer.save_pretrained(best_model_path)\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_accuracy': best_val_accuracy\n",
    "        }, os.path.join(best_model_path, \"best_text_model.pth\"))\n",
    "        print(f\"âœ… Best model saved with Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "print(\"âœ… Training Completed!\")\n",
    "\n",
    "\n",
    "# Load the best model for evaluation\n",
    "print(\"\\nðŸ”„ Loading the best model...\")\n",
    "best_model = BertForSequenceClassification.from_pretrained(best_model_path)\n",
    "best_tokenizer = BertTokenizer.from_pretrained(best_model_path)\n",
    "best_model.to(device)\n",
    "print(\"âœ… Best model loaded successfully!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1939b2-9a5e-42bd-abe3-e5fe662f03de",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8470ebb0-16cc-4b87-988a-163cad83d0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "evaluate_model(test_dataloader, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cd76d0-f6a4-4fb8-b6c1-1fdabe9f6a66",
   "metadata": {},
   "source": [
    "**After training, you can load the model and make predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35b8ca5-8d6b-4295-b6fb-3f5b55f6d3f1",
   "metadata": {},
   "source": [
    "## Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd11db1c-2ba0-470c-8240-1d5b06c558b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def classification_report_test(model, dataloader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating on Test Set\"):\n",
    "            batch = {\n",
    "                \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "                \"labels\": batch[\"label\"].to(device),  # Ensure correct label key\n",
    "            }\n",
    "            outputs = model(**batch)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "            all_preds.extend(predictions.cpu().numpy())  # Convert to NumPy for sklearn\n",
    "            all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(all_labels, all_preds, target_names=[\"Real\", \"Fake\"])\n",
    "    print(\"\\nðŸ“Š Classification Report on Test Set:\\n\")\n",
    "    print(report)\n",
    "\n",
    "classification_report_test(model, test_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0ec274-6fe7-48a0-a559-b38bca9c2a6c",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "\n",
    "* Precision: How many of the predicted \"Fake\" news are actually fake?\n",
    "\n",
    "* Recall: How many actual \"Fake\" news were correctly identified?\n",
    "\n",
    "* F1-score: The harmonic mean of precision & recall.\n",
    "\n",
    "* Support: Number of test samples in each class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6748bfa1-b7c9-4db2-9712-53be60ed7356",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e3fa23-3ae7-4af3-bd54-5e82a0265c6b",
   "metadata": {},
   "source": [
    "# Image Analysis: Using ViTs for detecting manipulated images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff1854b-18c1-47b3-b43f-264b57701abf",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c921c9d-0b35-464a-b4c7-ad401d2dd14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "import os\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import json\n",
    "import psutil\n",
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "\n",
    "\n",
    "# # First, create a directory to save models if it doesn't exist\n",
    "# save_dir = 'image_model_checkpoints'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f39970-a420-4638-b2e0-5cbdda659a1a",
   "metadata": {},
   "source": [
    "### Prepare to Load the Image Manipulation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f2f7b5-f184-48b4-8c49-143e6c76456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize image processor\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    \"google/vit-base-patch16-224\",\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "# Disable PIL's size limit\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "def print_memory_usage():\n",
    "    \"\"\"Monitor memory usage\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print(f\"Memory usage: {process.memory_info().rss / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "def preprocess_image(image, max_size=1024):\n",
    "    \"\"\"Process a single image with size limiting\"\"\"\n",
    "    try:\n",
    "        # Ensure image is in RGB format\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        \n",
    "        # Get image dimensions\n",
    "        width, height = image.size\n",
    "        \n",
    "        # Resize if the image is too large\n",
    "        if width > max_size or height > max_size:\n",
    "            aspect_ratio = width / height\n",
    "            if width > height:\n",
    "                new_width = max_size\n",
    "                new_height = int(max_size / aspect_ratio)\n",
    "            else:\n",
    "                new_height = max_size\n",
    "                new_width = int(max_size * aspect_ratio)\n",
    "            image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "        \n",
    "        # Process the image\n",
    "        processed = image_processor(image, return_tensors=\"pt\")\n",
    "        return processed.pixel_values[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {e}\")\n",
    "        return None\n",
    "\n",
    "class StreamingImageDataset(Dataset):\n",
    "    def __init__(self, dataset_split, buffer_size=1000):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with a buffer to store samples\n",
    "        Args:\n",
    "            dataset_split: The dataset split (train or test)\n",
    "            buffer_size: Number of samples to keep in memory\n",
    "        \"\"\"\n",
    "        self.dataset = dataset_split\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = []\n",
    "        self.current_index = 0\n",
    "        \n",
    "        # Fill initial buffer\n",
    "        self._fill_buffer()\n",
    "    \n",
    "    def _fill_buffer(self):\n",
    "        \"\"\"Fill the buffer with processed images\"\"\"\n",
    "        print(\"Filling data buffer...\")\n",
    "        self.buffer = []\n",
    "        \n",
    "        for i, item in enumerate(self.dataset.take(self.buffer_size)):\n",
    "            processed_image = preprocess_image(item['image'])\n",
    "            if processed_image is not None:\n",
    "                self.buffer.append((processed_image, item['label']))\n",
    "            \n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Processed {i + 1} images\")\n",
    "                print_memory_usage()\n",
    "        \n",
    "        print(f\"Buffer filled with {len(self.buffer)} images\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.buffer[idx]\n",
    "\n",
    "def create_data_loaders(batch_size=32, buffer_size=1000):\n",
    "    \"\"\"Create data loaders with streaming datasets\"\"\"\n",
    "    print(\"Loading dataset...\")\n",
    "    \n",
    "    try:\n",
    "        # Load dataset\n",
    "        dataset = load_dataset(\"date3k2/raw_real_fake_images\", streaming=True)\n",
    "        \n",
    "        # Create streaming datasets\n",
    "        train_dataset = StreamingImageDataset(dataset['train'], buffer_size=buffer_size)\n",
    "        test_dataset = StreamingImageDataset(dataset['test'], buffer_size=buffer_size)\n",
    "        \n",
    "        print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "        print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        return train_loader, test_loader\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating data loaders: {e}\")\n",
    "        raise\n",
    "\n",
    "def train_model(model, train_loader, test_loader, num_epochs=3, save_dir='image_model_results'):\n",
    "    \"\"\"Train the model with checkpointing and monitoring\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Initialize optimizer and criterion\n",
    "    optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "        \n",
    "        # Training loop with progress bar\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        for batch_idx, (images, labels) in enumerate(progress_bar):\n",
    "            print_memory_usage()  # Monitor memory usage\n",
    "            \n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(pixel_values=images)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{running_loss/(batch_idx+1):.4f}',\n",
    "                'accuracy': f'{correct/total:.4f}'\n",
    "            })\n",
    "            \n",
    "            # Clear some memory\n",
    "            del outputs, loss\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = correct / total\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_accuracy = evaluate_model(model, test_loader, device)\n",
    "        \n",
    "        # Save checkpoint if best model\n",
    "        if test_accuracy > best_accuracy:\n",
    "            best_accuracy = test_accuracy\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'accuracy': test_accuracy,\n",
    "            }\n",
    "            torch.save(checkpoint, os.path.join(save_dir, 'best_image_model.pth'))\n",
    "            print(f\"âœ… New best model saved with accuracy: {test_accuracy:.4f}\")\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Training Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_accuracy:.4f}\")\n",
    "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc='Evaluating'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(pixel_values=images)\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            # Clear some memory\n",
    "            del outputs\n",
    "            gc.collect()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "    \n",
    "print(\"âœ… Prepared to Load Data and Train Model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07773284-4d78-4730-9100-e2118c62fa9b",
   "metadata": {},
   "source": [
    "### Train and Evaluate the Image Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18359437-9731-4fc7-947b-9e1124c2941e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    BATCH_SIZE = 16\n",
    "    BUFFER_SIZE = 1000  # Adjust based on your available memory\n",
    "    NUM_EPOCHS = 3\n",
    "    SAVE_DIR = 'image_model_results'\n",
    "    \n",
    "    try:\n",
    "        # Create data loaders\n",
    "        print(\"Initializing data loaders...\")\n",
    "        train_loader, test_loader = create_data_loaders(\n",
    "            batch_size=BATCH_SIZE,\n",
    "            buffer_size=BUFFER_SIZE\n",
    "        )\n",
    "        print(\"âœ… DataLoaders Ready!\")\n",
    "        \n",
    "        # Initialize model\n",
    "        print(\"Loading Pretrained Vision Transformer (ViT) Model...\")\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = AutoModelForImageClassification.from_pretrained(\n",
    "            \"google/vit-base-patch16-224\",\n",
    "            num_labels=2,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        model.to(device)\n",
    "        print(\"âœ… Vision Transformer (ViT) Model Loaded!\")\n",
    "        \n",
    "        # Train model\n",
    "        train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            test_loader=test_loader,\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            save_dir=SAVE_DIR\n",
    "        )\n",
    "        \n",
    "        # Final evaluation\n",
    "        print(\"\\nPerforming final evaluation...\")\n",
    "        final_accuracy = evaluate_model(model, test_loader, device)\n",
    "        print(f\"Final Test Accuracy: {final_accuracy:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b613367d-b476-439c-8725-1a28b4b5cb14",
   "metadata": {},
   "source": [
    "**Evaluating Learning Curve Rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b34d5fc-fe5f-48bc-a67c-a17d57a2db0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, train_loader, test_loader, learning_rates, device, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Train and evaluate model across different learning rates\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for lr in learning_rates:\n",
    "        print(f\"\\nTraining with learning rate: {lr}\")\n",
    "        \n",
    "        # Reset model or create new instance\n",
    "        model.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
    "        \n",
    "        # Initialize optimizer and criterion\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Training loop\n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0\n",
    "            for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "                images, labels = batch\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass - note we need to access .logits\n",
    "                outputs = model(images).logits\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            # Print epoch statistics\n",
    "            epoch_loss = running_loss / len(train_loader)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        # Evaluate and store results\n",
    "        print(f\"\\nEvaluating model with learning rate {lr}\")\n",
    "        results[lr] = evaluate_model_performance(model, test_loader, device)\n",
    "    \n",
    "    # Plot comparison\n",
    "    plot_learning_rate_comparison(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def evaluate_model_performance(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate model performance with comprehensive metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_pred_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            images, labels = batch\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Get model outputs - note the .logits attribute\n",
    "            outputs = model(images).logits\n",
    "            \n",
    "            # Apply softmax to get probabilities\n",
    "            pred_probs = torch.softmax(outputs, dim=1)\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            # Store predictions and labels\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_pred_probs.extend(pred_probs[:, 1].cpu().numpy())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    y_pred = np.array(all_preds)\n",
    "    y_true = np.array(all_labels)\n",
    "    y_pred_prob = np.array(all_pred_probs)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    # Calculate ROC curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\nModel Performance Metrics:\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"AUC Score: {roc_auc:.4f}\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # 1. Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1)\n",
    "    ax1.set_title('Confusion Matrix')\n",
    "    ax1.set_ylabel('True Label')\n",
    "    ax1.set_xlabel('Predicted Label')\n",
    "    \n",
    "    # 2. ROC Curve\n",
    "    ax2.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "             label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    ax2.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    ax2.set_xlim([0.0, 1.0])\n",
    "    ax2.set_ylim([0.0, 1.05])\n",
    "    ax2.set_xlabel('False Positive Rate')\n",
    "    ax2.set_ylabel('True Positive Rate')\n",
    "    ax2.set_title('ROC Curve')\n",
    "    ax2.legend(loc=\"lower right\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': roc_auc,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "def plot_learning_rate_comparison(results_dict):\n",
    "    \"\"\"\n",
    "    Plot performance comparison across different learning rates\n",
    "    \n",
    "    Parameters:\n",
    "    results_dict: dictionary with learning rates as keys and performance metrics as values\n",
    "    \"\"\"\n",
    "    learning_rates = list(results_dict.keys())\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc']\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for metric in metrics:\n",
    "        values = [results_dict[lr][metric] for lr in learning_rates]\n",
    "        plt.semilogx(learning_rates, values, 'o-', label=metric.capitalize())\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Learning Rate')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Model Performance Metrics vs Learning Rate')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize model\n",
    "        model = AutoModelForImageClassification.from_pretrained(\n",
    "            \"google/vit-base-patch16-224\",\n",
    "            num_labels=2,\n",
    "            ignore_mismatched_sizes=True\n",
    "        ).to(device)\n",
    "        \n",
    "        # Define learning rates to try\n",
    "        learning_rates = [2e-5, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "        \n",
    "        # Train and evaluate\n",
    "        results = train_and_evaluate(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            test_loader=test_loader,\n",
    "            learning_rates=learning_rates,\n",
    "            device=device,\n",
    "            num_epochs=3\n",
    "        )\n",
    "        \n",
    "        # Print best results\n",
    "        best_lr = max(results.keys(), key=lambda x: results[x]['accuracy'])\n",
    "        print(f\"\\nBest learning rate: {best_lr}\")\n",
    "        print(\"====Best model performance metrics====\")\n",
    "        print(f\"Accuracy: {results[best_lr]['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {results[best_lr]['precision']:.4f}\")\n",
    "        print(f\"Recall: {results[best_lr]['recall']:.4f}\")\n",
    "        print(f\"F1 Score: {results[best_lr]['f1']:.4f}\")\n",
    "        print(f\"AUC Score: {results[best_lr]['auc']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during training and evaluation: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "print(\"Learning rate Evaluation Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2d9bd8-a1d1-4e06-86f6-f6c7ec810d01",
   "metadata": {},
   "source": [
    "### Performance of the Image Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b73048-2d34-4e33-b4d9-dfd5eb71142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "def collect_predictions(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Collect predictions and labels from the model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_pred_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Collecting predictions\"):\n",
    "            images, labels = batch\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Get model outputs - note the .logits attribute\n",
    "            outputs = model(images).logits\n",
    "            \n",
    "            # Apply softmax to get probabilities\n",
    "            pred_probs = torch.softmax(outputs, dim=1)\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            # Store predictions and labels\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_pred_probs.extend(pred_probs[:, 1].cpu().numpy())\n",
    "            \n",
    "    return np.array(all_preds), np.array(all_labels), np.array(all_pred_probs)\n",
    "\n",
    "def evaluate_model_performance(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate model performance with comprehensive metrics\n",
    "    \"\"\"\n",
    "    # Collect predictions\n",
    "    y_pred, y_true, y_pred_prob = collect_predictions(model, test_loader, device)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    # Calculate ROC curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\nModel Performance Metrics:\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"AUC Score: {roc_auc:.4f}\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # 1. Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1)\n",
    "    ax1.set_title('Confusion Matrix')\n",
    "    ax1.set_ylabel('True Label')\n",
    "    ax1.set_xlabel('Predicted Label')\n",
    "    \n",
    "    # 2. ROC Curve\n",
    "    ax2.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "             label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    ax2.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    ax2.set_xlim([0.0, 1.0])\n",
    "    ax2.set_ylim([0.0, 1.05])\n",
    "    ax2.set_xlabel('False Positive Rate')\n",
    "    ax2.set_ylabel('True Positive Rate')\n",
    "    ax2.set_title('ROC Curve')\n",
    "    ax2.legend(loc=\"lower right\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': roc_auc,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize the model\n",
    "        model = AutoModelForImageClassification.from_pretrained(\n",
    "            \"google/vit-base-patch16-224\",\n",
    "            num_labels=2,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        # Load the best saved model\n",
    "        checkpoint_path = os.path.join('image_model_results', 'best_image_model.pth')\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        print(\"\\nEvaluating model performance...\")\n",
    "        results = evaluate_model_performance(model, test_loader, device)\n",
    "        \n",
    "        # Print detailed results\n",
    "        print(\"\\nDetailed Performance Metrics:\")\n",
    "        print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {results['precision']:.4f}\")\n",
    "        print(f\"Recall: {results['recall']:.4f}\")\n",
    "        print(f\"F1 Score: {results['f1']:.4f}\")\n",
    "        print(f\"AUC Score: {results['auc']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in evaluation: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfea0034-d5a5-448f-9e5b-aedeb23c60c8",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11716f56-1ea9-40c8-a79e-9d1ca6792372",
   "metadata": {},
   "source": [
    "# Implementation for Audio Analysis (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76d1805-59e0-413b-8df7-649e0153aefd",
   "metadata": {},
   "source": [
    "### Load and Split Audio dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c43576-0514-47f9-94d0-ab02957f5303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Load the entire training dataset\n",
    "full_dataset = load_dataset(\"012shin/fake-audio-detection-augmented2\", split=\"train\")\n",
    "\n",
    "print(f\"Total samples in full dataset: {len(full_dataset)}\")\n",
    "\n",
    "# Convert the dataset to a format suitable for splitting\n",
    "dataset_dict = full_dataset.to_dict()\n",
    "indices = list(range(len(full_dataset)))\n",
    "\n",
    "# Perform stratified split to maintain class distribution\n",
    "train_indices, test_indices = train_test_split(\n",
    "    indices,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=[item for item in full_dataset['label']]\n",
    ")\n",
    "\n",
    "# Create the train and test datasets\n",
    "train_data = full_dataset.select(train_indices)\n",
    "test_data = full_dataset.select(test_indices)\n",
    "\n",
    "print(\"\\nDataset Split Results:\")\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Testing samples: {len(test_data)}\")\n",
    "\n",
    "# Verify class distribution in both splits\n",
    "def print_class_distribution(dataset, name=\"\"):\n",
    "    labels = [item for item in dataset['label']]\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    print(f\"\\n{name} Split Distribution:\")\n",
    "    print(\"----------------------------\")\n",
    "    for label, count in zip(unique_labels, counts):\n",
    "        percentage = (count/len(dataset))*100\n",
    "        print(f\"Label {label}: {count} samples ({percentage:.2f}%)\")\n",
    "\n",
    "# Print distributions\n",
    "print_class_distribution(train_data, \"Training\")\n",
    "print_class_distribution(test_data, \"Testing\")\n",
    "\n",
    "# Optional: Save the splits for future use\n",
    "train_data.save_to_disk('train_split')\n",
    "test_data.save_to_disk('test_split')\n",
    "\n",
    "# Verify data structure\n",
    "print(\"\\nSample data structure:\")\n",
    "print(\"----------------------\")\n",
    "sample = train_data[0]\n",
    "for key, value in sample.items():\n",
    "    print(f\"{key}: {type(value)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4706556-a6cf-4e85-9655-fbc3719e7685",
   "metadata": {},
   "source": [
    "### Load Audio Dataset from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd567d22-d9ca-4998-850f-511e85d8febe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the saved splits\n",
    "train_data = load_from_disk('train_split')\n",
    "test_data = load_from_disk('test_split')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a23d41-535f-4d80-be44-b77f3cb56046",
   "metadata": {},
   "source": [
    "### Explore Audio Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d037c175-7fc5-44c2-90a6-2edadb871404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def analyze_class_distribution(data, split_name=\"\"):\n",
    "    # First, let's examine the structure of a single item\n",
    "    first_item = data[0]\n",
    "    print(f\"\\nFirst item label structure: {first_item['label']}\")\n",
    "    \n",
    "    # Convert dataset to a list of labels\n",
    "    # If labels are lists, take the first element\n",
    "    labels = []\n",
    "    for item in data:\n",
    "        if isinstance(item['label'], list):\n",
    "            labels.append(item['label'][0])  # Take first element if it's a list\n",
    "        else:\n",
    "            labels.append(item['label'])\n",
    "            \n",
    "    # Convert to numpy array\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Count unique labels\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    label_counts = pd.Series(counts, index=unique_labels)\n",
    "    \n",
    "    # Calculate percentages\n",
    "    total_samples = len(labels)\n",
    "    label_percentages = (label_counts / total_samples * 100).round(2)\n",
    "    \n",
    "    # Print distribution\n",
    "    print(f\"\\n{split_name} Split Distribution:\")\n",
    "    print(\"----------------------------\")\n",
    "    for label, count in label_counts.items():\n",
    "        percentage = label_percentages[label]\n",
    "        print(f\"Label {label}: {count} samples ({percentage}%)\")\n",
    "    \n",
    "    # Create pie chart\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.pie(label_counts, labels=[f'Label {i}' for i in label_counts.index], \n",
    "            autopct='%1.1f%%', startangle=90,\n",
    "            colors=['lightblue', 'lightgreen'])\n",
    "    plt.title(f'Class Distribution - {split_name} Split')\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate imbalance ratio\n",
    "    if len(label_counts) > 1:\n",
    "        imbalance_ratio = label_counts.max() / label_counts.min()\n",
    "        print(f\"\\nImbalance Ratio: {imbalance_ratio:.2f}\")\n",
    "        if imbalance_ratio > 1.5:\n",
    "            print(\"âš ï¸ Dataset is imbalanced (ratio > 1.5)\")\n",
    "    \n",
    "    return label_counts\n",
    "\n",
    "# Basic dataset info\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Training Samples: {len(train_data)}\")\n",
    "print(f\"Testing Samples: {len(test_data)}\")\n",
    "\n",
    "# Analyze training data\n",
    "train_distribution = analyze_class_distribution(train_data, \"Training\")\n",
    "\n",
    "# Analyze test data\n",
    "test_distribution = analyze_class_distribution(test_data, \"Test\")\n",
    "\n",
    "# Compare distributions between train and test\n",
    "plt.figure(figsize=(10, 5))\n",
    "train_props = train_distribution / len(train_data)\n",
    "test_props = test_distribution / len(test_data)\n",
    "\n",
    "bar_width = 0.35\n",
    "index = range(len(train_props))\n",
    "\n",
    "plt.bar(index, train_props, bar_width, label='Train', color='lightblue')\n",
    "plt.bar([i + bar_width for i in index], test_props, bar_width, label='Test', color='lightgreen')\n",
    "\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Proportion')\n",
    "plt.title('Class Distribution Comparison - Train vs Test')\n",
    "plt.xticks([i + bar_width/2 for i in index], [f'Label {i}' for i in train_props.index])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print sample data structure\n",
    "print(\"\\nSample Data Structure:\")\n",
    "print(\"----------------------\")\n",
    "sample_item = train_data[0]\n",
    "for key, value in sample_item.items():\n",
    "    print(f\"{key}: {type(value)} - {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f624c0c1-105b-4446-8f87-57c7365e51d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio.transforms as transforms\n",
    "\n",
    "# Define transformation: Convert audio to Mel Spectrogram\n",
    "mel_transform = transforms.MelSpectrogram(\n",
    "    sample_rate=16000, n_mels=64, n_fft=1024, hop_length=512\n",
    ")\n",
    "\n",
    "# Define a fixed spectrogram length (128 frames)\n",
    "FIXED_SPEC_LENGTH = 128\n",
    "\n",
    "# Function to preprocess audio\n",
    "def preprocess(example):\n",
    "    waveform = example[\"audio\"][\"array\"]  # Extract waveform\n",
    "    waveform = torch.tensor(waveform, dtype=torch.float32).unsqueeze(0)  # Convert to float32 & add batch dimension\n",
    "    \n",
    "    mel_spec = mel_transform(waveform)  # Convert to Mel spectrogram\n",
    "\n",
    "    # Pad or truncate to fixed length\n",
    "    if mel_spec.shape[2] < FIXED_SPEC_LENGTH:\n",
    "        pad_amount = FIXED_SPEC_LENGTH - mel_spec.shape[2]\n",
    "        mel_spec = torch.nn.functional.pad(mel_spec, (0, pad_amount))  # Pad along time dimension\n",
    "    else:\n",
    "        mel_spec = mel_spec[:, :, :FIXED_SPEC_LENGTH]  # Truncate\n",
    "\n",
    "    example[\"mel_spectrogram\"] = mel_spec.squeeze(0)  # Remove batch dimension\n",
    "    return example\n",
    "\n",
    "# Apply preprocessing\n",
    "train_data = train_data.map(preprocess)\n",
    "test_data = test_data.map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6674364f-b2e2-4510-bddd-a7965dc01f7a",
   "metadata": {},
   "source": [
    "Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814136f5-48ce-4b21-8651-e016ede3bc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # First convert each mel_spectrogram to a tensor\n",
    "    mel_specs = [torch.FloatTensor(item[\"mel_spectrogram\"]) for item in batch]\n",
    "    mel_specs = torch.stack(mel_specs)\n",
    "    \n",
    "    # Convert labels to class indices (not one-hot)\n",
    "    # If your labels are already one-hot encoded, convert them to indices\n",
    "    labels = [torch.argmax(torch.tensor(item[\"label\"])) if isinstance(item[\"label\"], (list, np.ndarray)) \n",
    "             else item[\"label\"] for item in batch]\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    return mel_specs, labels\n",
    "\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_data, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_data, batch_size=8, collate_fn=collate_fn)\n",
    "\n",
    "print(\"âœ… DataLoaders Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb585f6-c47a-4ff2-b6e0-10f873ed6def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim=64, hidden_dim=128, num_layers=2, num_classes=2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        \n",
    "        x = x.permute(0, 2, 1)  # Reshape for LSTM (batch, seq, feature)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # Take last timestep output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de612df6-e801-4564-a06e-3c80252158f5",
   "metadata": {},
   "source": [
    "### Train & Save The Best Audio Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36423a49-1bf5-4a98-b6b8-c965e73661b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n",
    "# Define model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMModel().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop with best model saving\n",
    "num_epochs = 5\n",
    "best_accuracy = 0.0\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        mel_specs, labels = batch\n",
    "        # Move tensors to device\n",
    "        mel_specs = mel_specs.to(device)\n",
    "        labels = labels.to(device)  # Now labels should be 1D tensor of indices\n",
    "\n",
    "        # Reshape if needed\n",
    "        if len(mel_specs.shape) == 2:\n",
    "            mel_specs = mel_specs.unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        try:\n",
    "            outputs = model(mel_specs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during training: {str(e)}\")\n",
    "            print(f\"mel_specs shape: {mel_specs.shape}\")\n",
    "            print(f\"labels shape: {labels.shape}\")\n",
    "            print(f\"labels dtype: {labels.dtype}\")\n",
    "            print(f\"outputs shape: {outputs.shape if 'outputs' in locals() else 'N/A'}\")\n",
    "            print(f\"outputs dtype: {outputs.dtype if 'outputs' in locals() else 'N/A'}\")\n",
    "            raise e\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': epoch_loss,\n",
    "            'accuracy': accuracy\n",
    "        }, \"best_lstm_fake_audio.pth\")\n",
    "        print(\"ðŸ“ Best model saved!\")\n",
    "\n",
    "print(\"âœ… Training Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16219dc7-6426-4cd6-b1cd-a5060cf07662",
   "metadata": {},
   "source": [
    "### Evaluation and Performance of the Audio Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cbced7-8f54-49ff-8346-f1c2b8c9e430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            for mel_specs, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "                # Move data to device\n",
    "                mel_specs = mel_specs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(mel_specs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                # Collect predictions and labels\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "                # Calculate running accuracy\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "                \n",
    "        # Calculate metrics\n",
    "        accuracy = correct / total\n",
    "        \n",
    "        # Convert to numpy arrays for sklearn metrics\n",
    "        all_preds = np.array(all_preds)\n",
    "        all_labels = np.array(all_labels)\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "        recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "        # Create confusion matrix\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\n=== Model Evaluation Results ===\")\n",
    "        print(f\"ðŸ“Š Test Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"ðŸ“Š Precision: {precision:.4f}\")\n",
    "        print(f\"ðŸ“Š Recall: {recall:.4f}\")\n",
    "        print(f\"ðŸ“Š F1 Score: {f1:.4f}\")\n",
    "        print(\"\\n=== Confusion Matrix ===\")\n",
    "        print(cm)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'confusion_matrix': cm,\n",
    "            'predictions': all_preds,\n",
    "            'true_labels': all_labels\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {str(e)}\")\n",
    "        print(f\"Last batch shapes - mel_specs: {mel_specs.shape}, labels: {labels.shape}\")\n",
    "        raise e\n",
    "\n",
    "# Load and evaluate the model\n",
    "try:\n",
    "    # Load the best model\n",
    "    checkpoint = torch.load(\"best_lstm_fake_audio.pth\")\n",
    "    if isinstance(checkpoint, dict):\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Loaded model from epoch {checkpoint['epoch']} with training accuracy: {checkpoint['accuracy']:.4f}\")\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "    \n",
    "    # Make sure model is on the correct device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Evaluate\n",
    "    results = evaluate_model(model, test_loader, device)\n",
    "    \n",
    "    # Optional: Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(results['confusion_matrix'], annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading or evaluating model: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1bbdf5-8d8d-4a35-b4fc-28e338c09449",
   "metadata": {},
   "source": [
    "# 2. Model Selection & Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34203a5-d91b-48e2-952a-c58c86d81d67",
   "metadata": {},
   "source": [
    "## Text Classification (Fake News Detection with BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3875050a-7ced-45bc-aab9-092a4bacb37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# Load trained model and tokenizer\n",
    "print(\"\\nðŸ”„ Loading the best model...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(best_model_path)\n",
    "text_model = BertForSequenceClassification.from_pretrained(best_model_path)\n",
    "text_model.to(device)\n",
    "text_model.eval()\n",
    "print(\"âœ… Best model loaded successfully!\")\n",
    "\n",
    "\n",
    "# Predict function\n",
    "def predict(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = text_model(**inputs)\n",
    "    predicted_class = torch.argmax(outputs.logits).item()\n",
    "    return \"Fake News\" if predicted_class == 0 else \"Real News\"\n",
    "\n",
    "# Test prediction\n",
    "text = \"Can you blame her for losing her cool with Crooked Lying Hillary? Share this with all of your  undecided  friends!https://youtu.be/CCkXOix0g2Y\" # Fake News\n",
    "print(\"Prediction:\", predict(text))\n",
    "\n",
    "text = \"These people will support just about anything Obama says scary stuff!\" # Fake News\n",
    "print(\"Prediction:\", predict(text))\n",
    "\n",
    "text = \"Two people were shot dead on Friday as Kenyan police tried to disperse opposition supporters marching from an airport alongside the convoy of opposition leader Raila Odinga, a Reuters witness said.\" # Real News\n",
    "print(\"Prediction:\", predict(text))\n",
    "\n",
    "text = \"The North Korean issue should be resolved peacefully through talks, Chinese President Xi Jinping told British Prime Minister Theresa May in a telephone call, state radio said on Monday.\" # Real News\n",
    "print(\"Prediction:\", predict(text)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea36753-1d01-4807-a6a6-bae571aec2f9",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862a1045-6897-4fda-86b9-a935d8abbc22",
   "metadata": {},
   "source": [
    "## Image Classification (ViT-based Fake Image Detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5cf18a-366d-4c67-8b79-d8204f9e3203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ViTForImageClassification\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "print(\"\\nðŸ”„ Loading the best model...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize ViT model\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "# Modify the classifier for binary classification (Fake/Real)\n",
    "model.classifier = nn.Linear(model.config.hidden_size, 2)\n",
    "\n",
    "# Load the checkpoint\n",
    "save_dir = 'image_model_results'\n",
    "best_model_path = os.path.join(save_dir, 'best_image_model.pth')\n",
    "checkpoint = torch.load(best_model_path, map_location=device, weights_only=True)\n",
    "\n",
    "# Load state dict\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def predict_image(image_tensor, model):\n",
    "    \"\"\"\n",
    "    Make prediction on a single image\n",
    "    \"\"\"\n",
    "    image_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor).logits\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        confidence, predicted = torch.max(probabilities, dim=1)\n",
    "        \n",
    "        return predicted.item(), confidence.item()\n",
    "\n",
    "def load_and_preprocess_image(image_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess a specific image for prediction\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # ViT specific transformations\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                              std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        \n",
    "        # Load and transform image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image_tensor = transform(image)\n",
    "        \n",
    "        print(f\"âœ… Successfully loaded image: {image_path}\")\n",
    "        return image_tensor.to(device)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading image: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Test specific image\n",
    "print(\"\\nðŸ”„ Making prediction on fake_news_image.jpeg...\")\n",
    "image_path = 'fake_news_image.jpeg'\n",
    "\n",
    "if os.path.exists(image_path):\n",
    "    # Load and preprocess the image\n",
    "    test_tensor = load_and_preprocess_image(image_path)\n",
    "    \n",
    "    if test_tensor is not None:\n",
    "        # Make prediction\n",
    "        prediction, confidence = predict_image(test_tensor, model)\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\nResults:\")\n",
    "        print(f\"Image: {image_path}\")\n",
    "        print(f\"Prediction: {'Fake' if prediction == 0 else 'Real'}\")\n",
    "        print(f\"Confidence: {confidence:.2%}\")\n",
    "    else:\n",
    "        print(\"âŒ Failed to process the image.\")\n",
    "else:\n",
    "    print(f\"âŒ Image not found: {image_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77940afb-4b82-48f3-b14a-610a17d11cad",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9624cab-4416-45fb-a5a8-c27b2d41ff5a",
   "metadata": {},
   "source": [
    "## Multimodal Fusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1431ce98-3acb-4aef-86fe-9bf0981d0484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from torchvision import models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ViTForImageClassification\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ðŸ”¹ Load Text Model (BERT)\n",
    "class FakeTextDetector(nn.Module):\n",
    "    def __init__(self, model_path=\"best_bert_fake_news_detector\"):\n",
    "        super(FakeTextDetector, self).__init__()\n",
    "\n",
    "        # Load BERT model & tokenizer\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "        # Load additional checkpoint if exists\n",
    "        checkpoint_path = os.path.join(model_path, \"best_text_model.pth\")\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)\n",
    "            self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "            print(f\"âœ… Loaded trained text model from {model_path}\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Warning: Text model checkpoint not found!\")\n",
    "\n",
    "        self.model.to(device)\n",
    "\n",
    "    # def forward(self, text_inputs):\n",
    "    #     return self.model(**text_inputs).logits\n",
    "\n",
    "    def forward(self, text_inputs):\n",
    "        # Modified forward method to handle the input dictionary\n",
    "        outputs = self.model(\n",
    "            input_ids=text_inputs['input_ids'],\n",
    "            attention_mask=text_inputs.get('attention_mask', None),\n",
    "            token_type_ids=text_inputs.get('token_type_ids', None)\n",
    "        )\n",
    "        return outputs.logits\n",
    "\n",
    "\n",
    "# ðŸ”¹ Load Image Model (ViT)\n",
    "class FakeImageDetector(nn.Module):\n",
    "    def __init__(self, save_dir='image_model_results', model_name='best_image_model.pth'):\n",
    "        super(FakeImageDetector, self).__init__()\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Initialize transformations\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                              std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "\n",
    "        \n",
    "        # Initialize the ViT model\n",
    "        self.vit = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "        # Modify classifier for binary classification (Fake/Real)\n",
    "        self.vit.classifier = nn.Linear(self.vit.config.hidden_size, 2)\n",
    "        \n",
    "        # Setup model path\n",
    "        self.model_path = os.path.join(save_dir, model_name)\n",
    "        \n",
    "        # Load trained model\n",
    "        try:\n",
    "            if os.path.exists(self.model_path):\n",
    "                checkpoint = torch.load(self.model_path, map_location=device, weights_only=True)\n",
    "                state_dict = checkpoint['model_state_dict']\n",
    "                \n",
    "                print(f\"âœ… Loaded trained ViT model from {self.model_path}\")    \n",
    "            else:\n",
    "                print(f\"âš ï¸ Warning: ViT model checkpoint not found at {self.model_path}\")\n",
    "                print(\"Model will be initialized with random weights.\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading model: {str(e)}\")\n",
    "            print(\"Model will be initialized with random weights.\")\n",
    "            \n",
    "        # Move model to device\n",
    "        self.to(device)\n",
    "        self.eval()  # Set to evaluation mode by default\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass that returns logits for fusion model\n",
    "        \"\"\"\n",
    "        if len(x.shape) != 4:\n",
    "            raise ValueError(f\"Expected 4D tensor with shape [batch_size, channels, height, width], got shape {x.shape}\")\n",
    "        if x.shape[1] != 3:\n",
    "            raise ValueError(f\"Expected 3 channels, got {x.shape[1]}\")        \n",
    "        outputs = self.vit(x)\n",
    "        return outputs.logits\n",
    "\n",
    "    def predict(self, image_tensor):\n",
    "        \"\"\"\n",
    "        Standalone prediction method with confidence scores\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self(image_tensor)\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            confidence, predicted = torch.max(probabilities, dim=1)\n",
    "            return predicted.item(), confidence.item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ðŸ”¹ Load Audio Model (LSTM)\n",
    "class FakeAudioDetector(nn.Module):\n",
    "    def __init__(self, audio_model_path=\"best_lstm_fake_audio.pth\", input_size=64, hidden_size=128, num_layers=2):\n",
    "        super(FakeAudioDetector, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 2)  # Ensure output has 2 neurons (Fake/Real)\n",
    "\n",
    "        # Load trained model\n",
    "        if os.path.exists(audio_model_path):\n",
    "            checkpoint = torch.load(audio_model_path, weights_only=True, map_location=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "            \n",
    "            if \"model_state_dict\" in checkpoint:  # Ensure compatibility\n",
    "                self.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "                print(f\"âœ… Loaded trained audio model from {audio_model_path}\")\n",
    "            else:\n",
    "                self.load_state_dict(checkpoint)\n",
    "                print(\"âš ï¸ Warning: Model loaded without additional metadata.\")\n",
    "\n",
    "        else:\n",
    "            print(\"âš ï¸ Warning: Audio model checkpoint not found!\")\n",
    "\n",
    "        self.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        return self.fc(hidden[-1])\n",
    "\n",
    "\n",
    "\n",
    "# ðŸ”¹ Multimodal Fake News Detector (Combining Text, Image & Audio)\n",
    "class MultimodalFakeNewsDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultimodalFakeNewsDetector, self).__init__()\n",
    "\n",
    "        # Load individual models\n",
    "        self.text_model = FakeTextDetector()\n",
    "        self.image_model = FakeImageDetector()\n",
    "        self.audio_model = FakeAudioDetector()\n",
    "\n",
    "        # Fusion Layer (Combining all 3 modal features)\n",
    "        self.fc = nn.Linear(2 + 2 + 2, 2)  # Final classification\n",
    "\n",
    "    def forward(self, text_inputs, image_inputs, audio_inputs):\n",
    "        text_features = self.text_model(text_inputs)  # BERT output\n",
    "        image_features = self.image_model(image_inputs)  # ResNet output\n",
    "        audio_features = self.audio_model(audio_inputs)  # LSTM output\n",
    "\n",
    "        fused_features = torch.cat((text_features, image_features, audio_features), dim=1)\n",
    "        return self.fc(fused_features)\n",
    "\n",
    "\n",
    "# ðŸ”¹ Load the multimodal model\n",
    "multimodal_model = MultimodalFakeNewsDetector()\n",
    "multimodal_model.eval()  # Set to evaluation mode\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a6a2ba-a42e-4f54-9b59-d7735b8b6371",
   "metadata": {},
   "source": [
    "# 3. Multimodal Model Training & Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6b388b-a38d-4d21-bd49-2dd4c3cd5817",
   "metadata": {},
   "source": [
    "### Dataset for the multimodal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427c97ff-2ec9-435e-9ab1-cd97ae915588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, recall_score, f1_score, precision_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from timm import create_model\n",
    "import os\n",
    "\n",
    "def check_model_files():\n",
    "    model_files = [\n",
    "        'best_bert_fake_news_detector/best_text_model.pth',\n",
    "        'image_model_results/best_image_model.pth',\n",
    "        'best_lstm_fake_audio.pth'\n",
    "    ]\n",
    "    \n",
    "    for file_path in model_files:\n",
    "        if not os.path.isfile(file_path):\n",
    "            print(f\"Warning: {file_path} not found\")\n",
    "        else:\n",
    "            print(f\"Found: {file_path}\")\n",
    "\n",
    "# inspect model files\n",
    "check_model_files()\n",
    "\n",
    "\n",
    "def inspect_audio_dataset():\n",
    "    print(\"Inspecting audio dataset structure...\")\n",
    "    audio_data = load_dataset(\"012shin/fake-audio-detection-augmented\", split='train')\n",
    "    \n",
    "    # Get first item\n",
    "    first_item = audio_data[0]\n",
    "    print(\"\\nKeys in first audio item:\", first_item.keys())\n",
    "    print(\"\\nSample audio item structure:\", first_item)\n",
    "    \n",
    "    return first_item.keys()\n",
    "\n",
    "# inspect audio dataset\n",
    "audio_keys = inspect_audio_dataset()\n",
    "print(f\"\\nAvailable audio keys: {audio_keys}\")\n",
    "\n",
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, split='train'):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        print(f\"Loading {split} datasets...\")\n",
    "        \n",
    "        try:\n",
    "            # Load datasets from Hugging Face\n",
    "            print(\"Loading text dataset...\")\n",
    "            self.text_data = load_dataset(\"BeardedJohn/FakeNews\", split=split)\n",
    "            # self.text_data = load_dataset(\"ErfanMoosaviMonazzah/fake-news-detection-dataset-English\", split=split)\n",
    "            \n",
    "            print(\"Loading image dataset...\")\n",
    "            self.image_data = load_dataset(\"itsLeen/deepfake_vs_real_image_detection\", split=split)\n",
    "            # self.image_data = load_dataset(\"date3k2/raw_real_fake_images\", split=split)\n",
    "            \n",
    "            print(\"Loading audio dataset...\")\n",
    "            self.audio_data = load_dataset(\"012shin/fake-audio-detection-augmented\", split=split)\n",
    "            # self.audio_data = load_dataset(\"012shin/fake-audio-detection-augmented2\", split=split)\n",
    "            \n",
    "            print(\"All datasets loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading datasets: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "        # Define transforms\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.text_data), len(self.image_data), len(self.audio_data))\n",
    "\n",
    "    @staticmethod\n",
    "    def ensure_batch_dimension(tensor, expected_shape):\n",
    "        \"\"\"\n",
    "        Ensures tensor has batch dimension and correct shape\n",
    "        \"\"\"\n",
    "        if tensor.dim() == len(expected_shape):\n",
    "            # Add batch dimension if missing\n",
    "            tensor = tensor.unsqueeze(0)\n",
    "        \n",
    "        current_shape = tuple(tensor.shape[1:])  # Shape without batch dimension\n",
    "        if current_shape != expected_shape:\n",
    "            raise ValueError(f\"Expected shape {expected_shape}, got {current_shape}\")\n",
    "        \n",
    "        return tensor\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            # Process text\n",
    "            text = self.text_data[idx]['text']\n",
    "            text_encoding = self.tokenizer(\n",
    "                text,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Process image\n",
    "            try:\n",
    "                # Get image data\n",
    "                image_data = self.image_data[idx]['image']\n",
    "                \n",
    "                # Convert to PIL Image if it's not already\n",
    "                if isinstance(image_data, dict) and 'bytes' in image_data:\n",
    "                    image = Image.open(io.BytesIO(image_data['bytes'])).convert('RGB')\n",
    "                elif isinstance(image_data, str):\n",
    "                    # If it's a file path\n",
    "                    image = Image.open(image_data).convert('RGB')\n",
    "                elif isinstance(image_data, Image.Image):\n",
    "                    image = image_data.convert('RGB')\n",
    "                else:\n",
    "                    # If it's raw bytes\n",
    "                    image = Image.open(io.BytesIO(image_data)).convert('RGB')\n",
    "                \n",
    "                image_tensor = self.image_transform(image)\n",
    "                image_tensor = MultimodalDataset.ensure_batch_dimension(image_tensor, (3, 224, 224))\n",
    "                # print(\"Final image tensor shape:\", image_tensor.shape)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image at index {idx}: {str(e)}\")\n",
    "                image_tensor = torch.zeros(1, 3, 224, 224)  # Include batch dimension\n",
    "\n",
    "\n",
    "            # Process audio\n",
    "            try:\n",
    "                # Extract features from raw audio\n",
    "                audio_data = self.audio_data[idx]['audio']\n",
    "                audio_array = torch.tensor(audio_data['array'])\n",
    "                \n",
    "                # Generate features from the audio array\n",
    "                audio_features = self.extract_audio_features(audio_array)\n",
    "                \n",
    "                # Ensure correct dimensions using ensure_batch_dimension\n",
    "                audio_features = MultimodalDataset.ensure_batch_dimension(audio_features, (64,))\n",
    "                # print(\"Audio features shape:\", audio_features.shape)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing audio at index {idx}: {str(e)}\")\n",
    "                audio_features = torch.zeros(1, 64)  # Create fallback with batch dimension\n",
    "                        \n",
    "            \n",
    "            # Get labels\n",
    "            label = torch.tensor(self.text_data[idx]['label'])\n",
    "            \n",
    "            # Squeeze any extra dimensions from the text encoding\n",
    "            text_encoding = {k: v.squeeze(0) for k, v in text_encoding.items()}\n",
    "            \n",
    "            return {\n",
    "                'text': text_encoding,\n",
    "                'image': image_tensor,\n",
    "                'audio': audio_features,\n",
    "                'label': label\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing item {idx}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def inspect_item(self, idx):\n",
    "        \"\"\"Debug method to inspect raw data at a specific index\"\"\"\n",
    "        print(\"\\nInspecting raw data at index\", idx)\n",
    "        print(\"\\nText data:\", self.text_data[idx])\n",
    "        print(\"\\nImage data:\", self.image_data[idx])\n",
    "        print(\"\\nAudio data:\", self.audio_data[idx])\n",
    "\n",
    "\n",
    "    def extract_audio_features(self, audio_array):\n",
    "        \"\"\"\n",
    "        Extract features from raw audio array\n",
    "        You can choose one of these methods or combine them\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Option 1: Simple statistical features\n",
    "            features = []\n",
    "            chunk_size = len(audio_array) // 64  # Divide audio into 64 chunks\n",
    "            \n",
    "            for i in range(0, len(audio_array), chunk_size):\n",
    "                chunk = audio_array[i:i + chunk_size]\n",
    "                features.append(chunk.mean())\n",
    "            \n",
    "            features = torch.tensor(features)\n",
    "            \n",
    "            # Ensure we have exactly 64 features\n",
    "            if len(features) < 64:\n",
    "                features = F.pad(features, (0, 64 - len(features)))\n",
    "            elif len(features) > 64:\n",
    "                features = features[:64]\n",
    "                \n",
    "            return features\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in feature extraction: {str(e)}\")\n",
    "            return torch.zeros(64)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93c6ae1-92df-45cb-aa55-5a3e7ce64450",
   "metadata": {},
   "source": [
    "# 4. Evaluation & Performance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e240d77-2897-48ef-a22a-83031ab49a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Enable memory efficient settings\n",
    "torch.cuda.empty_cache()\n",
    "# Set memory allocator configuration\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "# Clear memory cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "class MultimodalFakeNewsDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        try:\n",
    "            print(\"Loading pre-trained models...\")\n",
    "\n",
    "            # Load individual models\n",
    "            print(\"âœ… Loading BERT model...\")\n",
    "            self.text_model = FakeTextDetector()\n",
    "            print(\"âœ… Loading LSTM model...\")\n",
    "            self.image_model = FakeImageDetector()\n",
    "            print(\"âœ… Loading trained audio model\")\n",
    "            self.audio_model = FakeAudioDetector()\n",
    "            print(\"âœ… Loaded trained models\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading models: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "        self.fusion_layer = nn.Linear(2 + 2 + 2, 2)  # Final classification\n",
    "\n",
    "    def forward(self, text_input, image_input, audio_input):\n",
    "        # Get embeddings from each model\n",
    "        text_output = self.text_model(**text_input).logits\n",
    "        \n",
    "        # Clear memory after text processing if needed\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        \n",
    "        image_output = self.image_model(image_input)\n",
    "        # Clear memory after text processing if needed\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        audio_output = self.audio_model(audio_input)\n",
    "        # Clear memory after text processing if needed\n",
    "        torch.cuda.empty_cache()        \n",
    "        \n",
    "        # Concatenate all features\n",
    "        combined = torch.cat((text_output, image_output, audio_output), dim=1)\n",
    "\n",
    "        # If intermediate outputs aren't needed anymore, delete them\n",
    "        del text_output\n",
    "        del image_output\n",
    "        del audio_output\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Final classification\n",
    "        output = self.fusion_layer(combined)\n",
    "        return output\n",
    "\n",
    "print(\"Ready to train model...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8bb3f7-113d-4c66-b95c-83935563ccad",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63413f0-a2e9-473e-8147-2c304332bf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(train_losses, val_losses, learning_rates):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot losses\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot learning rates\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(learning_rates)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Learning Rate Schedule')\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def train_multimodal_model(model, train_loader, val_loader, num_epochs=3, learning_rate=0.0001):\n",
    "    print(\"Initializing training...\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Enable gradient scaler - corrected initialization\n",
    "    scaler = torch.cuda.amp.GradScaler()  # Remove device_type parameter\n",
    "\n",
    "    # Set model to use float32\n",
    "    model = model.to(device).float()\n",
    "\n",
    "    # Initialize return values    \n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    learning_rates = []\n",
    "    accumulation_steps = 8\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    try:\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            for batch_idx, batch in enumerate(tqdm(train_loader)):\n",
    "                try:\n",
    "                    # Process in chunks with mixed precision\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        # Process each modality separately\n",
    "                        text_inputs = {k: v.to(device) for k, v in batch['text'].items()}\n",
    "                        # text_inputs = {k: v.to(device).long() for k, v in batch['text'].items()}\n",
    "\n",
    "                        # Debug print\n",
    "                        # if batch_idx == 0:\n",
    "                        #     print(\"\\nText input keys:\", text_inputs.keys())\n",
    "                        #     for k, v in text_inputs.items():\n",
    "                        #         print(f\"{k} shape:\", v.shape)\n",
    "                        \n",
    "                        text_outputs = model.text_model(text_inputs)\n",
    "                        del text_inputs\n",
    "                        torch.cuda.empty_cache()\n",
    "                        \n",
    "                        # Convert inputs to float32\n",
    "                        image_inputs = batch['image'].to(device).float()\n",
    "\n",
    "                        \n",
    "                        # Remove the extra dimension (squeeze dimension 1)\n",
    "                        image_inputs = image_inputs.squeeze(1)\n",
    "                        \n",
    "                        # print(\"\\nImage tensor details:\")\n",
    "                        # print(\"Shape after squeeze:\", image_inputs.shape)\n",
    "                        # print(\"Type:\", image_inputs.dtype)\n",
    "                        # print(\"Device:\", image_inputs.device)\n",
    "                        \n",
    "                        # Verify the shape is correct\n",
    "                        if len(image_inputs.shape) != 4:\n",
    "                            raise ValueError(f\"Expected 4D tensor, got shape {image_inputs.shape}\")\n",
    "                        if image_inputs.shape[1] != 3:\n",
    "                            raise ValueError(f\"Expected 3 channels, got {image_inputs.shape[1]}\")\n",
    "                        \n",
    "                        # Process image\n",
    "                        image_outputs = model.image_model(image_inputs)\n",
    "                        # print(\"Image outputs shape:\", image_outputs.shape)\n",
    "                        \n",
    "                        \n",
    "                        del image_inputs\n",
    "                        torch.cuda.empty_cache()\n",
    "                        \n",
    "                        # audio_inputs = batch['audio'].to(device)\n",
    "                        audio_inputs = batch['audio'].to(device).float()\n",
    "                        audio_outputs = model.audio_model(audio_inputs)\n",
    "                        del audio_inputs\n",
    "                        torch.cuda.empty_cache()\n",
    "                        \n",
    "                        # Combine outputs\n",
    "                        combined = torch.cat((text_outputs, image_outputs, audio_outputs), dim=1)\n",
    "                        outputs = model.fusion_layer(combined)\n",
    "                        del combined, text_outputs, image_outputs, audio_outputs\n",
    "                        \n",
    "                        # Calculate loss\n",
    "                        labels = batch['label'].to(device).long()\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        loss = loss / accumulation_steps\n",
    "                    \n",
    "                    # Backward pass with gradient scaling\n",
    "                    scaler.scale(loss).backward()\n",
    "                    \n",
    "                    if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                        optimizer.zero_grad()\n",
    "                    \n",
    "                    train_loss += loss.item() * accumulation_steps\n",
    "                    \n",
    "                    # Clean up\n",
    "                    del outputs, loss, labels\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "                except RuntimeError as e:\n",
    "                    if \"out of memory\" in str(e):\n",
    "                        print('| WARNING: ran out of memory, skipping batch')\n",
    "                        torch.cuda.empty_cache()\n",
    "                        optimizer.zero_grad()\n",
    "                        continue\n",
    "                    else:\n",
    "                        raise e\n",
    "            \n",
    "\n",
    "            # Rest of the training loop remains the same...\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            train_losses.append(avg_train_loss)\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "            \n",
    "            with torch.no_grad():  # This prevents gradient calculation\n",
    "                for batch_idx, batch in enumerate(val_loader):\n",
    "                    try:\n",
    "                        # Process text\n",
    "                        text_inputs = {k: v.to(device) for k, v in batch['text'].items()}\n",
    "                        text_outputs = model.text_model(text_inputs)\n",
    "                        \n",
    "                        # Process image\n",
    "                        image_inputs = batch['image'].to(device).float()\n",
    "                        image_inputs = image_inputs.squeeze(1)\n",
    "                        image_outputs = model.image_model(image_inputs)\n",
    "                        \n",
    "                        # Process audio\n",
    "                        audio_inputs = batch['audio'].to(device).float()\n",
    "                        audio_outputs = model.audio_model(audio_inputs)\n",
    "                        \n",
    "                        # Combine outputs\n",
    "                        combined = torch.cat((text_outputs, image_outputs, audio_outputs), dim=1)\n",
    "                        outputs = model.fusion_layer(combined)\n",
    "                        \n",
    "                        # Calculate validation loss\n",
    "                        labels = batch['label'].to(device).long()\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        val_loss += loss.item()\n",
    "                        \n",
    "                        _, predicted = torch.max(outputs, 1)\n",
    "                        val_correct += (predicted == labels).sum().item()\n",
    "                        val_total += labels.size(0)\n",
    "\n",
    "                        \n",
    "                        # Clean up memory\n",
    "                        del outputs, text_outputs, image_outputs, audio_outputs, combined\n",
    "                        torch.cuda.empty_cache()\n",
    "                        \n",
    "                    except RuntimeError as e:\n",
    "                        if \"out of memory\" in str(e):\n",
    "                            print('| WARNING: ran out of memory during validation, skipping batch')\n",
    "                            torch.cuda.empty_cache()\n",
    "                            continue\n",
    "                        else:\n",
    "                            print(f\"Runtime error in validation: {str(e)}\")\n",
    "                            raise\n",
    "                \n",
    "                # Calculate average validation loss\n",
    "                avg_val_loss = val_loss / len(val_loader)\n",
    "                val_losses.append(avg_val_loss)\n",
    "                val_accuracy = val_correct / val_total if val_total > 0 else 0\n",
    "                \n",
    "                # Learning rate scheduling\n",
    "                scheduler.step(avg_val_loss)\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                learning_rates.append(current_lr)\n",
    "                \n",
    "                print(f'Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}, LR = {current_lr}, Accuracy: {val_accuracy:.4f}')\n",
    "                \n",
    "                # Save best model\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': best_val_loss,\n",
    "                    }, 'best_multimodal_model.pth')\n",
    "            \n",
    "            # Clear cache after each epoch\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        print(f\"Training complete. Returning {len(train_losses)} epochs of data\")\n",
    "        return train_losses, val_losses, learning_rates\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in training loop: {str(e)}\")\n",
    "        print(f\"Error type: {type(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        # Even if there's an error, return what we have\n",
    "        return train_losses, val_losses, learning_rates\n",
    "    \n",
    "print(\"Training function ready!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83485f6-14d0-4d2a-bef8-b0b8dd4d60fa",
   "metadata": {},
   "source": [
    "Execute training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d6baab-0f79-46e9-ba29-4b3a528eb141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, recall_score, f1_score, precision_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from timm import create_model\n",
    "import os\n",
    "\n",
    "def main():\n",
    "    # Set smaller batch size and enable memory optimization\n",
    "    BATCH_SIZE = 4  # Further reduced batch size\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    try:\n",
    "        # Create datasets with lazy loading\n",
    "        print(\"Loading datasets...\")\n",
    "        train_dataset = MultimodalDataset(split='train')\n",
    "        test_dataset = MultimodalDataset(split='test')\n",
    "        \n",
    "        # Create dataloaders with minimal memory footprint\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            shuffle=True,\n",
    "            num_workers=1,  # Reduced workers\n",
    "            pin_memory=False,  # Disabled pin_memory\n",
    "            prefetch_factor=1\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size=BATCH_SIZE,\n",
    "            num_workers=1,\n",
    "            pin_memory=False,\n",
    "            prefetch_factor=1\n",
    "        )\n",
    "        \n",
    "        # Initialize model with memory optimization\n",
    "        print(\"\\nInitializing multimodal model...\")\n",
    "        model = MultimodalFakeNewsDetector()\n",
    "        \n",
    "        # Move model to CPU first\n",
    "        model = model.cpu()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # Print memory status\n",
    "        print(\"\\nGPU Memory Before Loading Model:\")\n",
    "        print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "        print(f\"Cached: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "        \n",
    "        # Move model to GPU in parts if needed\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if torch.cuda.is_available():\n",
    "            # Move individual components to GPU\n",
    "            model.text_model = model.text_model.to(device)\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            model.image_model = model.image_model.to(device)\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            model.audio_model = model.audio_model.to(device)\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            model.fusion_layer = model.fusion_layer.to(device)\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        print(\"\\nGPU Memory After Loading Model:\")\n",
    "        print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "        print(f\"Cached: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "        \n",
    "\n",
    "        # Train model with memory optimization\n",
    "        print(\"Starting training process...\")\n",
    "        results = train_multimodal_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            num_epochs=3,\n",
    "            learning_rate=0.0001\n",
    "        )\n",
    "\n",
    "        print(\"Training completed. Processing results...\")\n",
    "        # print(f\"Results type: {type(results)}\")\n",
    "        \n",
    "        if not isinstance(results, tuple):\n",
    "            raise ValueError(f\"Expected tuple from train_multimodal_model, got {type(results)}\")\n",
    "            \n",
    "        if len(results) != 3:\n",
    "            raise ValueError(f\"Expected 3 values, got {len(results)}\")\n",
    "            \n",
    "        train_losses, val_losses, learning_rates = results\n",
    "\n",
    "        print(\"Training completed. Processing results...\")\n",
    "        print(f\"Number of epochs completed: {len(train_losses)}\")\n",
    "        print(f\"Final training loss: {train_losses[-1]:.4f}\")\n",
    "        print(f\"Final validation loss: {val_losses[-1]:.4f}\")\n",
    "        \n",
    "        return train_losses, val_losses, learning_rates  \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main: {str(e)}\")\n",
    "        print(f\"Error type: {type(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        # Even if there's an error, return what we have\n",
    "        return train_losses, val_losses, learning_rates\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        print(\"Starting main execution...\")\n",
    "        results = main()\n",
    "        \n",
    "        if not isinstance(results, tuple) or len(results) != 3:\n",
    "            raise ValueError(f\"Invalid results format: {type(results)}\")\n",
    "            \n",
    "        train_losses, val_losses, learning_rates = results\n",
    "        print(\"\\nTraining completed successfully!\")\n",
    "        \n",
    "        # Plot results\n",
    "        plot_training_curves(train_losses, val_losses, learning_rates)\n",
    "   \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Fatal error: {str(e)}\")\n",
    "        print(f\"Error type: {type(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "print(\"\\n multimodal model PoC completed!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df55e48-b1fe-410d-84a7-89d4e12d3d9b",
   "metadata": {},
   "source": [
    "### MultiModal Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda0247f-9893-4c34-bab9-8b441cb8d28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multimodal_model(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the multimodal model's performance metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    print(\"Starting evaluation...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            try:\n",
    "                # Process text\n",
    "                text_inputs = {k: v.to(device) for k, v in batch['text'].items()}\n",
    "                text_outputs = model.text_model(text_inputs)\n",
    "                \n",
    "                # Process image\n",
    "                image_inputs = batch['image'].to(device).float()\n",
    "                image_inputs = image_inputs.squeeze(1)\n",
    "                image_outputs = model.image_model(image_inputs)\n",
    "                \n",
    "                # Process audio\n",
    "                audio_inputs = batch['audio'].to(device).float()\n",
    "                audio_outputs = model.audio_model(audio_inputs)\n",
    "                \n",
    "                # Combine outputs\n",
    "                combined = torch.cat((text_outputs, image_outputs, audio_outputs), dim=1)\n",
    "                outputs = model.fusion_layer(combined)\n",
    "                \n",
    "                # Get predictions\n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                \n",
    "                # Move to CPU and convert to numpy\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(batch['label'].cpu().numpy())\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "                \n",
    "                # Clean up memory\n",
    "                del outputs, text_outputs, image_outputs, audio_outputs, combined\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error processing batch: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    # Calculate ROC curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(all_labels, all_probs[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Generate classification report\n",
    "    target_names = ['Fake', 'Real']  \n",
    "    report = classification_report(\n",
    "        all_labels, \n",
    "        all_preds, \n",
    "        target_names=target_names,\n",
    "        digits=4,\n",
    "        output_dict=True\n",
    "    )    \n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nModel Performance Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print detailed report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(\"=====================\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=target_names, digits=4))\n",
    "    \n",
    "   \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': cm,\n",
    "        'roc_auc': roc_auc,\n",
    "        'predictions': all_preds,\n",
    "        'probabilities': all_probs\n",
    "    }\n",
    "\n",
    "# Usage example:\n",
    "def main():\n",
    "    # Initialize your model and test_loader here\n",
    "    model = MultimodalFakeNewsDetector()\n",
    "    test_dataset = MultimodalDataset(split='test')\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "        num_workers=1,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    # Load the best model weights\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    checkpoint = torch.load('best_multimodal_model.pth', map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    results = evaluate_multimodal_model(model, test_loader, device)\n",
    "    \n",
    "    # Save results to file\n",
    "    with open('multimodal_evaluation_results.txt', 'w') as f:\n",
    "        f.write(\"Multimodal Model Evaluation Results\\n\")\n",
    "        f.write(\"==================================\\n\")\n",
    "        f.write(f\"ðŸ“Š Accuracy: {results['accuracy']:.4f}\\n\")\n",
    "        f.write(f\"ðŸ“Š Precision: {results['precision']:.4f}\\n\")\n",
    "        f.write(f\"ðŸ“Š Recall: {results['recall']:.4f}\\n\")\n",
    "        f.write(f\"ðŸ“Š F1 Score: {results['f1']:.4f}\\n\")\n",
    "        f.write(f\"ðŸ“Š ROC AUC: {results['roc_auc']:.4f}\\n\")\n",
    "\n",
    "        \n",
    "    return train_losses, val_losses, learning_rates\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ff219d-ffea-48f9-a320-68f0b7ba630a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
